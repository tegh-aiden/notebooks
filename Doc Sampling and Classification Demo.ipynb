{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f69c4164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6fa30b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2cbacb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\Tegh\\\\Documents\\\\TweetData\\\\full-corpus.csv\")\n",
    "df = df.sample(frac=1)\n",
    "corpus = np.array(df['TweetText'])\n",
    "labels = np.array(df['Topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1524a6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Tweets = 3660\n",
      "Spanish Tweets = 672\n",
      "French Tweets = 205\n"
     ]
    }
   ],
   "source": [
    "langid.set_languages(['en','es','fr','pt','nl','it','de'])  # ISO 639-1 codes\n",
    "codeDict = {'en' : 'english','es' : 'spanish','fr' : 'french','pt' : 'portuguese',\n",
    "            'nl' : 'dutch','it' : 'italian','de' : 'german'}\n",
    "\n",
    "langArr = np.array([codeDict[langid.classify(doc)[0]] for doc in corpus])\n",
    "\n",
    "print('English Tweets = ' + str((langArr == 'english').sum()))\n",
    "print('Spanish Tweets = ' + str((langArr == 'spanish').sum()))\n",
    "print('French Tweets = ' + str((langArr == 'french').sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e62f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize words\n",
    "corpus = [word_tokenize(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91aaeb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming, removing stop words, numbers, punctuation\n",
    "for lang in codeDict.values():\n",
    "    stops = set(stopwords.words(lang))\n",
    "    stemmer = SnowballStemmer(lang)\n",
    "    for index in np.where(langArr == lang)[0]:\n",
    "        corpus[index] = \" \".join([stemmer.stem(w) for w in corpus[index] if (w.isalpha() and w not in stops)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0496bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Tf-idf sparse matrix\n",
    "vectorizer = TfidfVectorizer(min_df=2,max_df=0.5,ngram_range=(1,2), max_features = 1000)\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "vocab = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9db8127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randSample(corpus, labels, pct_acq, pct_del):\n",
    "    n = int((pct_acq + pct_del) * len(corpus))\n",
    "    indices = np.random.choice(len(corpus), n, replace=False)\n",
    "    X = [corpus[i] for i in indices]\n",
    "    Y = [labels[i] for i in indices]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff3a3b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y_probs):\n",
    "     return -1.0 * np.sum(y_probs * np.log(y_probs + np.finfo(float).eps)) / np.log(y_probs.size)\n",
    "    \n",
    "def least_confidence(y_probs):\n",
    "     return y_probs.size * (1 - np.nanmax(y_probs)) / (y_probs.size - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93e93108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7643262272638373\n",
      "0.625073342460395\n",
      "0.5757872090749071\n",
      "0.560923137101506\n",
      "0.549970663015842\n",
      "0.4901232153334637\n",
      "0.488362996283982\n",
      "0.48406023860747116\n",
      "0.48327791902992373\n",
      "0.48288675924115\n"
     ]
    }
   ],
   "source": [
    "Iterations = 10\n",
    "pct_acq = 0.02\n",
    "pct_del = 0.0\n",
    "metric = 'entropy'\n",
    "\n",
    "X, Y = randSample(corpus, labels, pct_acq, pct_del)\n",
    "vectorizer = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1,2), max_features = 1000)\n",
    "tfidf_matrix = vectorizer.fit_transform(X)\n",
    "model = MultinomialNB()\n",
    "model.fit(tfidf_matrix, Y)\n",
    "y_probs = model.predict_proba(vectorizer.transform(corpus))\n",
    "del vectorizer\n",
    "del model\n",
    "for itr in range(10):\n",
    "    if metric == 'LC':\n",
    "        uncertainty = pd.DataFrame([least_confidence(y) for y in y_probs]).sort_values(by = 0, ascending = False, axis = 0)\n",
    "    elif metric == 'entropy':\n",
    "        uncertainty = pd.DataFrame([entropy(y) for y in y_probs]).sort_values(by = 0, ascending = False, axis = 0)\n",
    "    n = int((pct_acq + pct_del) * len(corpus))\n",
    "    X.extend([corpus[i] for i in uncertainty.iloc[:n].index.tolist()])\n",
    "    Y.extend([labels[i] for i in uncertainty.iloc[:n].index.tolist()])\n",
    "    vectorizer = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1,2), max_features = 1000)\n",
    "    tfidf_matrix = vectorizer.fit_transform(X)\n",
    "    model = MultinomialNB()\n",
    "    model.fit(tfidf_matrix, Y)\n",
    "    print(model.score(vectorizer.transform(corpus), labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
