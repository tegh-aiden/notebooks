{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "981fa2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import langid\n",
    "import matplotlib.pyplot as plt\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f2b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import EnglishStemmer, SpanishStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aea6e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load documents in fasttext format\n",
    "raw_train = \"C:\\\\Users\\\\Tegh\\\\fasttext\\\\amazon_reviews\\\\train.ft.txt\"\n",
    "raw_test = \"C:\\\\Users\\\\Tegh\\\\fasttext\\\\amazon_reviews\\\\test.ft.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12255e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "langid.set_languages(['en','es'])  # ISO 639-1 codes\n",
    "EngStemmer = EnglishStemmer()\n",
    "EngStops = set(stopwords.words('english'))\n",
    "\n",
    "EspStemmer = SpanishStemmer()\n",
    "EspStops = set(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9fb26c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import mmap\n",
    "\n",
    "def get_num_lines(file_path):\n",
    "    fp = open(file_path, \"r+\")\n",
    "    buf = mmap.mmap(fp.fileno(), 0)\n",
    "    lines = 0\n",
    "    while buf.readline():\n",
    "        lines += 1\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a19824e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|██▍                                                                                                                  | 76574/3600000 [04:37<2:03:22, 475.97it/s]"
     ]
    }
   ],
   "source": [
    "#Expect precision and recall of 0.916 if all is in order for full train\n",
    "#Throwout reviews with encoding errors\n",
    "\n",
    "count = 0\n",
    "f_train = open(raw_train, 'r')\n",
    "out_train = open('C:\\\\Users\\\\Tegh\\\\fasttext\\\\amazon_reviews\\\\out_train.txt', 'w')\n",
    "\n",
    "for i in tqdm(range(get_num_lines(raw_train))):\n",
    "    try:\n",
    "        line = f_train.readline()\n",
    "        lang = langid.classify(line)[0]\n",
    "        if not line:\n",
    "            break\n",
    "        line = line.split(\" \", 1)\n",
    "        line[1] = word_tokenize(line[1])\n",
    "        if lang == 'en':\n",
    "            line[1] = \" \".join([EngStemmer.stem(w) for w in line[1] if (w.isalpha() and w not in EngStops)])\n",
    "        else:\n",
    "            line[1] = \" \".join([EspStemmer.stem(w) for w in line[1] if (w.isalpha() and w not in EspStops)])\n",
    "        out_train.write(line[0] + \" \" + line[1] + \"\\n\")\n",
    "    except Exception as e:\n",
    "        count += 1\n",
    "        continue\n",
    "f_train.close()\n",
    "out_train.close()\n",
    "print(\"Exceptions in train set: \" + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bb80eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                           | 17/400000 [00:00<25:48, 258.30it/s]\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x81 in position 282: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-641da95c110d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mout_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:\\\\Users\\\\Tegh\\\\fasttext\\\\amazon_reviews\\\\out_test.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mget_num_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mlang\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlangid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tegh\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tegh\\appdata\\local\\programs\\python\\python37\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x81 in position 282: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "f_test = open(raw_test, 'r')\n",
    "out_test = open('C:\\\\Users\\\\Tegh\\\\fasttext\\\\amazon_reviews\\\\out_test.txt', 'w')\n",
    "\n",
    "for line in tqdm(f_test, total=get_num_lines(raw_test)):\n",
    "    try:\n",
    "        lang = langid.classify(line)[0]\n",
    "        if not line:\n",
    "            break\n",
    "        line = line.split(\" \", 1)\n",
    "        line[1] = word_tokenize(line[1])\n",
    "        if lang == 'en':\n",
    "            line[1] = \" \".join([EngStemmer.stem(w) for w in line[1] if (w.isalpha() and w not in EngStops)])\n",
    "        else:\n",
    "            line[1] = \" \".join([EspStemmer.stem(w) for w in line[1] if (w.isalpha() and w not in EspStops)])\n",
    "        out_test.write(line[0] + \" \" + line[1] + \"\\n\")\n",
    "    except Exception as e:\n",
    "        count += 1\n",
    "        continue\n",
    "f_test.close()\n",
    "out_test.close()\n",
    "print(\"Exceptions in test set: \" + str(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d51e95f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__label__2'\n",
      " list(['stune', 'even', 'this', 'sound', 'track', 'beauti', 'it', 'paint', 'seneri', 'mind', 'well', 'i', 'would', 'recomend', 'even', 'peopl', 'hate', 'vid', 'game', 'music', 'i', 'play', 'game', 'chrono', 'cross', 'game', 'i', 'ever', 'play', 'best', 'music', 'it', 'back', 'away', 'crude', 'keyboard', 'take', 'fresher', 'step', 'grate', 'guitar', 'soul', 'orchestra', 'it', 'would', 'impress', 'anyon', 'care', 'listen'])]\n"
     ]
    }
   ],
   "source": [
    "def randSample(docs, pct_acq, pct_del = 0):\n",
    "    n = int((pct_acq + pct_del) * len(docs))\n",
    "    indices = np.random.choice(len(docs), n, replace=False)\n",
    "    return [docs[i] for i in indices]\n",
    "\n",
    "def dropout(X, Y, pct_acq, pct_del):\n",
    "    n = int(pct_acq / (pct_acq + pct_del) * len(X))\n",
    "    indices = np.random.choice(len(X), n, replace=False)\n",
    "    return [X[i] for i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675657ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y_probs):\n",
    "    return -1.0 * np.sum(y_probs * np.log(y_probs + np.finfo(float).eps)) / np.log(y_probs.size)\n",
    "    \n",
    "def least_confidence(y_probs):\n",
    "    return y_probs.size * (1 - np.nanmax(y_probs)) / (y_probs.size - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81870a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasttext(iters, pct_acq, metric, pct_del = 0):\n",
    "    accuracy = list()\n",
    "    X = randSample(x_train, pct_acq, pct_del)\n",
    "    if pct_del > 0:\n",
    "        X = dropout(X, pct_acq, pct_del)\n",
    "    model = train_supervised('train.txt', autotuneValidationFile='valid.txt')\n",
    "    y_probs = model.predict(doc for doc in train)\n",
    "    for itr in range(iters):\n",
    "        if metric == 'LC':\n",
    "            uncertainty = pd.DataFrame([least_confidence(y) for y in y_probs]).sort_values(by = 0, ascending = False, axis = 0)\n",
    "        elif metric == 'entropy':\n",
    "            uncertainty = pd.DataFrame([entropy(y) for y in y_probs]).sort_values(by = 0, ascending = False, axis = 0)\n",
    "        n = int((pct_acq + pct_del) * len(train))\n",
    "        subX = [train[i] for i in uncertainty.iloc[:n].index.tolist()]\n",
    "        if pct_del > 0:\n",
    "            subX = dropout(subX, pct_acq, pct_del)\n",
    "        #Append lines to txt file\n",
    "        X.extend(subX)\n",
    "        model = train_supervised('train.txt', autotuneValidationFile='valid.txt')\n",
    "        y_probs = model.predict(doc for doc in train)\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
