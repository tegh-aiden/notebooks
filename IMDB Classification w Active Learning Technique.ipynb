{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b69f4949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import langid\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49c2ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e2f16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "np.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning)\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(num_words = 25000)\n",
    "X = np.concatenate((x_train, x_test))\n",
    "Y = np.concatenate((y_train, y_test))\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c32d24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maps i to ith most common word in dataset\n",
    "word_index = tf.keras.datasets.imdb.get_word_index()\n",
    "inverted_word_index = dict((i, word) for (word, i) in word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "365f1c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "\n",
      "the and a of to is br in it i this that was as for with movie but film on not you are his have\n",
      "\n",
      "the and a of to is br in it i this that was as for with movie but film on not you are his have he be one all at by an they who so from like her or just about it's out has if some there what good more when very up no time she even my would which only story really see their had can were me well than we much been bad get will do also into people other first great because how him most don't made its then way make them too could any movies after\n"
     ]
    }
   ],
   "source": [
    "#Print x most frequent words\n",
    "def mostFreq(x):\n",
    "    return \" \".join(inverted_word_index[i] for i in range(1, x+1))\n",
    "\n",
    "print(mostFreq(1), end='\\n\\n')\n",
    "print(mostFreq(25), end='\\n\\n')\n",
    "print(mostFreq(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "660df049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22500\n",
      "2500\n"
     ]
    }
   ],
   "source": [
    "#Get 90:10 train-test split while maintaining 50-50 positive-negative ratio\n",
    "df = pd.DataFrame({'X': X, 'Y': Y})\n",
    "df = df.sort_values(by=['Y'])\n",
    "dataset = df.to_numpy()\n",
    "train = np.concatenate((dataset[:22500], dataset[25000:47500]))\n",
    "test = np.concatenate((dataset[22500:25000], dataset[47500:]))\n",
    "print(sum(train[i][1] for i in range(45000)))\n",
    "print(sum(test[i][1] for i in range(5000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72f53f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array([[inverted_word_index[word] for word in doc[0]] for doc in train])\n",
    "y_train = np.array([doc[1] for doc in train])\n",
    "x_test = np.array([[inverted_word_index[word] for word in doc[0]] for doc in test])\n",
    "y_test = np.array([doc[1] for doc in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d247377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming, removing stop words, numbers, punctuation\n",
    "stops = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "for i in range(len(x_train)):\n",
    "    x_train[i] = \" \".join([stemmer.stem(w) for w in x_train[i] if (w.isalpha() and w not in stops)])\n",
    "for i in range(len(x_test)):\n",
    "    x_test[i] = \" \".join([stemmer.stem(w) for w in x_test[i] if (w.isalpha() and w not in stops)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "237695e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Tf-idf sparse matrix\n",
    "vectorizer = TfidfVectorizer(min_df=2,max_df=0.5,ngram_range=(1,2), max_features = 10000)\n",
    "tfidf_matrix = vectorizer.fit_transform(x_train)\n",
    "vocab = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "03e43c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randSample(docs, targets, pct_acq, pct_del = 0):\n",
    "    n = int((pct_acq + pct_del) * len(docs))\n",
    "    indices = np.random.choice(len(docs), n, replace=False)\n",
    "    X = [docs[i] for i in indices]\n",
    "    Y = [targets[i] for i in indices]\n",
    "    return X, Y\n",
    "\n",
    "def dropout(X, Y, pct_acq, pct_del):\n",
    "    n = int(pct_acq / (pct_acq + pct_del) * len(X))\n",
    "    indices = np.random.choice(len(X), n, replace=False)\n",
    "    X = [X[i] for i in indices]\n",
    "    Y = [Y[i] for i in indices]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "41259acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y_probs):\n",
    "    return -1.0 * np.sum(y_probs * np.log(y_probs + np.finfo(float).eps)) / np.log(y_probs.size)\n",
    "    \n",
    "def least_confidence(y_probs):\n",
    "    return y_probs.size * (1 - np.nanmax(y_probs)) / (y_probs.size - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e23bf5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NB_with_TFIDF(iters, pct_acq, metric, pct_del = 0, features = 10000):\n",
    "    accuracy = list()\n",
    "    X, Y = randSample(x_train, y_train, pct_acq, pct_del)\n",
    "    if pct_del > 0:\n",
    "        X, Y = dropout(X, Y, pct_acq, pct_del)\n",
    "    vectorizer = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1,2), max_features = features)\n",
    "    tfidf_matrix = vectorizer.fit_transform(X)\n",
    "    model = MultinomialNB()\n",
    "    model.fit(tfidf_matrix, Y)\n",
    "    accuracy.append(model.score(vectorizer.transform(x_test), y_test))\n",
    "    y_probs = model.predict_proba(vectorizer.transform(x_train))\n",
    "    del vectorizer\n",
    "    del model\n",
    "    for itr in range(iters):\n",
    "        if metric == 'LC':\n",
    "            uncertainty = pd.DataFrame([least_confidence(y) for y in y_probs]).sort_values(by = 0, ascending = False, axis = 0)\n",
    "        elif metric == 'entropy':\n",
    "            uncertainty = pd.DataFrame([entropy(y) for y in y_probs]).sort_values(by = 0, ascending = False, axis = 0)\n",
    "        n = int((pct_acq + pct_del) * len(x_train))\n",
    "        subX = [x_train[i] for i in uncertainty.iloc[:n].index.tolist()]\n",
    "        subY = [y_train[i] for i in uncertainty.iloc[:n].index.tolist()]\n",
    "        if pct_del > 0:\n",
    "            subX, subY = dropout(subX, subY, pct_acq, pct_del)\n",
    "        X.extend(subX)\n",
    "        Y.extend(subY)\n",
    "        vectorizer = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1,2), max_features = features)\n",
    "        tfidf_matrix = vectorizer.fit_transform(X)\n",
    "        model = MultinomialNB()\n",
    "        model.fit(tfidf_matrix, Y)\n",
    "        score = model.score(vectorizer.transform(x_test), y_test)\n",
    "        accuracy.append(score)\n",
    "        y_probs = model.predict_proba(vectorizer.transform(x_train))\n",
    "        del vectorizer\n",
    "        del model\n",
    "    print(accuracy)\n",
    "    return accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
